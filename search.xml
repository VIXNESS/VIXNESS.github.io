<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title><![CDATA[Effective Modern C++ è¡Œä¸ºä¹ æƒ¯åˆ—ä¸¾]]></title>
    <url>%2F2019%2F05%2F07%2FEffective-Modern-C-%E8%A1%8C%E4%B8%BA%E4%B9%A0%E6%83%AF%E5%88%97%E4%B8%BE%2F</url>
    <content type="text"><![CDATA[Effective Modern C++ è¡Œä¸ºä¹ æƒ¯åˆ—ä¸¾ é¢å‘å·²ç»æŽŒæ¡äº†C++11ä¹‹å‰çš„åŒå­¦, æœ¬æ–‡åªåˆ—ä¸¾äº†åŸºæœ¬çš„å‡ æ¡. ref: Effective Modern C++ 1.ç”¨auto ä»£æ›¿æ˜¾ç¤ºå£°æ˜Ž. âœ– é”™è¯¯ç¤ºèŒƒ: 1typename std::iterator_traits&lt;It&gt;::value_type currValue = *b; âœ” æ­£ç¡®ç¤ºèŒƒ: 1auto currValue = *b; 2.ä½¿ç”¨nullptr æ›¿ä»£NULL å’Œ0. âœ– é”™è¯¯ç¤ºèŒƒ: 123if (result == 0)&#123; ...&#125; 123if (result == NULL)&#123; ...&#125; âœ” æ­£ç¡®ç¤ºèŒƒ: 123if (result == nullptr)&#123; ...&#125; 3.ä½¿ç”¨using æ›¿ä»£typedef. âœ– é”™è¯¯ç¤ºèŒƒ: 1typedef std::unique_ptr&lt;std::unordered_map&lt;std::string, std::string&gt;&gt; UPtrMapSS; âœ” æ­£ç¡®ç¤ºèŒƒ: 1using UPtrMapSS = std::unique_ptr&lt;std::unordered_map&lt;std::string, std::string&gt;&gt;; 4.æœ‰èŒƒå›´çš„enums æ›¿ä»£æ— èŒƒå›´çš„enums. âœ– é”™è¯¯ç¤ºèŒƒ: 12enum Color &#123; black, white, red &#125;;auto white = false; //white å·²ç»è¢«å£°æ˜Žäº†, error! âœ” æ­£ç¡®ç¤ºèŒƒ: 1234enum class Color &#123; black, white, red &#125;;auto white = false; // ä¸€åˆ‡æ­£å¸¸Color c = Color::white; //è§„èŒƒçš„å£°æ˜Žæ–¹å¼auto c = Color::white; //è§„èŒƒçš„å£°æ˜Žæ–¹å¼ 5.ç¦ç”¨å‡½æ•°æ—¶, ç”¨delete æ›¿ä»£private. âœ– é”™è¯¯ç¤ºèŒƒ: 12345678class basic_ios : public ios_base &#123; public: ...private: basic_ios(const basic_ios&amp; ); basic_ios&amp; operator=(const basic_ios&amp;); ...&#125;; âœ” æ­£ç¡®ç¤ºèŒƒ: 123456class basic_ios : public ios_base &#123; public: basic_ios(const basic_ios&amp; ) = delete; basic_ios&amp; operator=(const basic_ios&amp;) = delete; ...&#125;; 6.ä½¿ç”¨override å…³é”®å­—æ ‡æ³¨ overrideå‡½æ•°. 1234567891011121314class Base &#123; public: virtual void mf1() const; virtual void mf2(int x); virtual void mf3() &amp;; void mf4() const; &#125;;class Derived: public Base &#123; public: virtual void mf1() override; virtual void mf2(unsigned int x) override; virtual void mf3() &amp;&amp; override; virtual void mf4() const override;&#125;; 7.ä½¿ç”¨const_iterators æ›¿ä»£iterators. âœ– é”™è¯¯ç¤ºèŒƒ: 1234std::vector&lt;int&gt; values;â€¦auto it = std::find(values.begin(),values.end(), 1983); //ä½¿ç”¨begin()å’Œend()values.insert(it, 1998); âœ” æ­£ç¡®ç¤ºèŒƒ: 1234std::vector&lt;int&gt; values; â€¦auto it = std::find(values.cbegin(),values.cend(), 1983);//ä½¿ç”¨cbegin()å’Œcend()values.insert(it, 1998); 8.å¦‚æžœå‡½æ•°ä¸ä¼šæŠ›å‡ºå¼‚å¸¸, ä½¿ç”¨noexceptè¿›è¡Œå£°æ˜Ž. âœ– C++98: 1int f(int x) throw(); âœ” C++11: 1int f(int x) noexcept; 9.ä½¿ç”¨æ™ºèƒ½æŒ‡é’ˆ std::unique_ptr, std::shared_ptr, std::weak_ptræ›¿ä»£ä¼ ç»ŸæŒ‡é’ˆ (std::auto æ·˜æ±°äº†åˆ«ç”¨äº†). 10.èƒ½ç”¨constexprå°±ç”¨constexpr. 11.è®©å¸¸æˆå‘˜å‡½æ•°çº¿ç¨‹å®‰å…¨: ä½¿ç”¨std::mutex æˆ–std::atomic ç­‰. 12.å–„ç”¨å³å€¼[Rvalue], è¯­ä¹‰è½¬ç§»[Move Semantics], å®Œç¾Žè½¬å‘[Perfect Forwarding] 13.å–„ç”¨Lambda è¡¨è¾¾å¼ 14.å–„ç”¨å¹¶å‘ç¼–ç¨‹API 15.å®¹å™¨ä¸­ä½¿ç”¨emplace_back(), æ›¿ä»£push_back()]]></content>
      <categories>
        <category>c++</category>
      </categories>
      <tags>
        <tag>c++</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[DNN å¹´æ”¶å…¥é¢„æµ‹]]></title>
    <url>%2F2018%2F12%2F24%2F%E5%B9%B4%E6%94%B6%E5%85%A5%E9%A2%84%E6%B5%8B%2F</url>
    <content type="text"><![CDATA[æ”¶å…¥é¢„æµ‹ é€šè¿‡ èŒä½ã€å©šå§»æƒ…å†µã€å­¦åŽ†ã€å®¶åº­è§’è‰²ã€ç§æ—ã€æ€§åˆ«å¯¹å…¶è¿›è¡Œå¹´è–ªçš„é¢„æµ‹ðŸ¥³ðŸ¥³ Data SetTraining Data Testing Data é¢„å¤‡å·¥ä½œæ‰€éœ€ç±»åº“12345import tensorflow as tffrom tensorflow import kerasimport numpy as npimport sysimport csv æ•°æ®è£…è½½ æ•°æ®æž„æˆ: [feature 1, â€¦ feature n; &gt;= 50k or &lt; 50k] ðŸ˜µðŸ˜µðŸ˜µ123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177178179180181182def loadData(_x, _y, fileName): col_1 = [' State-gov', ' Self-emp-not-inc', ' Private', ' Federal-gov', ' Local-gov', ' ?', ' Self-emp-inc', ' Without-pay', ' Never-worked'] col_3 = [' Bachelors', ' HS-grad', ' 11th', ' Masters', ' 9th', ' Some-college', ' Assoc-acdm', ' Assoc-voc', ' 7th-8th', ' Doctorate', ' Prof-school', ' 5th-6th', ' 10th', ' 1st-4th', ' Preschool', ' 12th'] col_5 = [' Never-married', ' Married-civ-spouse', ' Divorced', ' Married-spouse-absent', ' Separated', ' Married-AF-spouse', ' Widowed'] col_6 = [' Adm-clerical', ' Exec-managerial', ' Handlers-cleaners', ' Prof-specialty', ' Other-service', ' Sales', ' Craft-repair', ' Transport-moving', ' Farming-fishing', ' Machine-op-inspct', ' Tech-support', ' ?', ' Protective-serv', ' Armed-Forces', ' Priv-house-serv'] col_7 = [' Not-in-family', ' Husband', ' Wife', ' Own-child', ' Unmarried', ' Other-relative'] col_8 = [' White', ' Black', ' Asian-Pac-Islander', ' Amer-Indian-Eskimo', ' Other'] col_9 = [' Male', ' Female'] col_13 = [' United-States', ' Cuba', ' Jamaica', ' India', ' ?', ' Mexico', ' South', ' Puerto-Rico', ' Honduras', ' England', ' Canada', ' Germany', ' Iran', ' Philippines', ' Italy', ' Poland', ' Columbia', ' Cambodia', ' Thailand', ' Ecuador', ' Laos', ' Taiwan', ' Haiti', ' Portugal', ' Dominican-Republic', ' El-Salvador', ' France', ' Guatemala', ' China', ' Japan', ' Yugoslavia', ' Peru', ' Outlying-US(Guam-USVI-etc)', ' Scotland', ' Trinadad&amp;Tobago', ' Greece', ' Nicaragua', ' Vietnam', ' Hong', ' Ireland', ' Hungary', ' Holand-Netherlands'] col_14 = [' &lt;=50K', ' &gt;50K', ' &lt;=50K.', ' &gt;50K.'] with open(fileName) as rawData: rows = csv.reader(rawData, delimiter = ",") for r in rows: if len(r) == 0: continue temp = [] for i in range(15): if i == 0: temp.append(float(r[i])) if i == 1: cnt = 0 for c in col_1: if c == r[i]: temp.append(float(cnt)) break cnt += 1 if i == 2: temp.append(float(r[i])) if i == 3: cnt = 0 for c in col_3: if c == r[i]: temp.append(float(cnt)) break cnt += 1 if i == 4: temp.append(float(r[i])) if i == 5: cnt = 0 for c in col_5: if c == r[i]: temp.append(float(cnt)) break cnt += 1 if i == 6: cnt = 0 for c in col_6: if c == r[i]: temp.append(float(cnt)) break cnt += 1 if i == 7: cnt = 0 for c in col_7: if c == r[i]: temp.append(float(cnt)) break cnt += 1 if i == 8: cnt = 0 for c in col_8: if c == r[i]: temp.append(float(cnt)) break cnt += 1 if i == 9: cnt = 0 for c in col_9: if c == r[i]: temp.append(float(cnt)) break cnt += 1 if i == 10 or i == 11 or i == 12: temp.append(float(r[i])) if i == 13: cnt = 0 for c in col_13: if c == r[i]: temp.append(float(cnt)) break cnt += 1 if i == 14: cnt = 0 for c in col_14: if c == r[i]: _y.append(float(cnt % 2)) break cnt += 1 _x.append(temp) æ¨¡åž‹å®šä¹‰Feature Scaling 4ç§ Feature Scalingçš„æ–¹æ³• Standardization12345678910111213141516def standardization(dataMatrix): #ðŸ¤©ðŸ¤©ðŸ¤© if dataMatrix.shape[0] == 0: return dataMatrix for i in range(dataMatrix.shape[1]): sum = 0 for _x in dataMatrix: sum += _x[i] mean = sum / dataMatrix.shape[0] SD = 0 for _x in dataMatrix: SD += (_x[i] - mean)**2 SD = np.sqrt(SD / dataMatrix.shape[0]) for _x in dataMatrix: _x[i] = (_x[i] - mean) / SD return dataMatrix Mean Normalization123456789101112131415161718def meanNormalization(dataMatrix): if dataMatrix.shape[0] == 0: return dataMatrix for i in range(dataMatrix.shape[1]): sum = 0 max = 0 min = 0 for data in dataMatrix: sum += data[i] if data[i] &gt; max: max = data[i] if data[i] &lt; min: min = data[i] mean = sum / dataMatrix.shape[0] if (max - min) != 0: for data in dataMatrix: data[i] = (data[i] - mean) / (max - min) return dataMatrix Rescaling123456789101112131415def rescaling(dataMatrix): if dataMatrix.shape[0] == 0: return dataMatrix for i in range(dataMatrix.shape[1]): max = 0 min = 0 for data in dataMatrix: if data[i] &gt; max: max = data[i] if data[i] &lt; min: min = data[i] if max - min != 0: for data in dataMatrix: data[i] = (data[i] - min) / (max - min) return dataMatrix ä½¿ç”¨ä»¥ä¸ŠæŸä¸€ç§æ–¹æ³•å¯¹æ•°æ®è¿›è¡ŒFeature Scalingç»è¿‡æµ‹è¯•,Standardizationçš„æ•ˆæžœæœ€å¥½,å…¶æ¬¡æ˜¯ Mean Normalization  12345678910111213141516trainX = []trainY = []testX = []testY = []loadData(trainX, trainY, 'train.csv')loadData(testX, testY, 'test.csv')trainX = np.array(trainX)trainY = np.array(trainY)testX = np.array(testX)testY = np.array(testY)trainX = standardization(trainX) #Test accuracy: 0.8504391622392502testX = standardization(testX)# trainX = meanNormalization(trainX) #Test accuracy: 0.8516675879786739# testX = meanNormalization(testX)# trainX = rescaling(trainX) #Test accuracy: 0.8477366255400303# testX = rescaling(testX) è®­ç»ƒè®¾è®¡æ¨¡åž‹ ä½¿ç”¨ä¸¤å±‚layers æ¿€æ´»å‡½æ•° ReLU (å°±ä¹±è®¾è®¡çš„) ðŸ˜…Loss Function ç”¨çš„æ˜¯Cross Entropy, å› ä¸ºä½¿ç”¨äº†ReLUç”¨Square Errorä¼šæœ‰è®¸å¤šçš„åœ°æ–¹æ²¡æœ‰æ¢¯åº¦,å¾ˆå°´å°¬ðŸ¥µ 12345678model = tf.keras.Sequential([ keras.layers.Dense(28, activation=tf.nn.relu), keras.layers.Dense(14, activation=tf.nn.relu), keras.layers.Dense(2,activation=tf.nn.softmax)])model.compile(optimizer=tf.train.AdamOptimizer(), loss='sparse_categorical_crossentropy', metrics=['accuracy']) è®­ç»ƒæ—¶çš„ç²¾å‡†åº¦: æµ‹è¯•ç²¾å‡†åº¦: 1216281/16281 [==============================] - 0s 25us/stepTest loss: 0.3315070253370084 Test accuracy: 0.8466310422863189 è´Ÿä¼˜åŒ–å–æ¶ˆFeature Scaling å¾ˆçœŸå®ž ðŸ¥µ 1216281/16281 [==============================] - 0s 24us/stepTest loss: 12.310577790542826 Test accuracy: 0.23622627602315244 å¢žåŠ layers123456model = tf.keras.Sequential([ keras.layers.Dense(28, activation=tf.nn.relu), keras.layers.Dense(28, activation=tf.nn.relu), keras.layers.Dense(14, activation=tf.nn.relu), keras.layers.Dense(2,activation=tf.nn.softmax)]) æ²¡ç”¨ðŸ¤¦â€ ðŸ¤¦â€ ðŸ¤·â€ ðŸ¤·â€ 1216281/16281 [==============================] - 0s 26us/stepTest loss: 0.33134235454223626 Test accuracy: 0.8481665745274117 å®Œç»“ðŸŽ‰ðŸŽ‰ðŸŽ‰ Repo&gt;50K?]]></content>
      <categories>
        <category>machine learning</category>
      </categories>
      <tags>
        <tag>machine learning</tag>
        <tag>deep learning</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[PM2.5 é¢„æµ‹]]></title>
    <url>%2F2018%2F12%2F24%2FPM2-5%2F</url>
    <content type="text"><![CDATA[ä½¿ç”¨Linear Regression å¯¹PM2.5è¿›è¡Œé¢„æµ‹æ•°æ®é›† training data testing data: samples testing data: label Training data å’Œ Public testing data çš„ç»„ç»‡å½¢å¼: ä¸€å¤©ç”±18è¡Œç»„æˆ,ä¸€è¡Œä¸ºä¸€ä¸ªæŒ‡æ ‡,ä¸€å…±ç”±18ä¸ªæŒ‡æ ‡,ä»Žç¬¬4åˆ—å¼€å§‹è®°å½•æ¯ä¸ªæŒ‡æ ‡ä¸€å¤©å†…24å°æ—¶çš„å˜åŒ–æ•°å€¼,æ¯ä¸ªæœˆè¿žç»­è®°å½•å‰20å¤©ä½œä¸ºtraining set,åŽ10å¤©ä½œä¸ºtesting set,ä¸€å…±è®°å½•äº†240ä¸ªå°æ—¶ æ—¥æœŸ è§‚æµ‹ç«™ æŒ‡æ ‡ 0æ—¶ â€¦ 23æ—¶ day 1 xxx PM2.5 day 1 xxx PM10 day 1 xxx SO2 day 1 xxx â€¦ day 2 xxx PM2.5 day 2 xxx PM10 day 2 xxx SO2 day 2 xxx â€¦ æ•°æ®å¤„ç†å¿…è¦ç±»åº“1234567import osimport tensorflow as tffrom tensorflow import kerasimport csvimport sysimport numpy as npimport matplotlib.pyplot as plt è½½å…¥æ•°æ®12345678910111213141516data = []for i in range(18): data.append([]) # åˆå§‹åŒ–18åˆ—n_row = 0with open('train.csv','r',encoding = 'big5') as text: #csvç¼–ç æ˜¯big5 row = csv.reader(text, delimiter = ",") for r in row: if n_row != 0: for i in range(3,27): if r[i] != "NR": #NRä¸ºæœªé™é›¨,å¯¹å…¶è®¾ç½®ä¸ºé™é›¨é‡ data[(n_row - 1) % 18].append(float(r[i])) else: #è®¾æ–½0é™é›¨ä¸ºä¸€ä¸ªæŽ¥è¿‘0çš„å°é‡, #è‹¥è®¾ä¸º0,åŽç»­çš„æ¢¯åº¦è®¡ç®—ä¼šæœ‰é™¤0çš„é£Žé™© data[(n_row - 1) % 18].append(float(0.0001)) n_row += 1 é‡æ–°ç»„ç»‡æ•°æ® å°†ä¹‹å‰çš„æ•°æ®é‡æ–°ç»„ç»‡,å¯¹æ¯ä¸ªå°æ—¶è¿›è¡Œè¿žç»­æ‹¼æŽ¥ Features 0æ—¶ â€¦ 23æ—¶ 0æ—¶(æ¬¡æ—¥) â€¦ 23æ—¶ PM2.5 â€¦ .. PM 10 123456789101112131415161718x = [] # æ ·æœ¬çŸ©é˜µy = [] # å®žé™…çš„å€¼for i in range(12): # 12ä¸ªæœˆ for j in range(471): # æ¯è¾“å…¥9ä¸ªå°æ—¶çš„æ•°å€¼,é¢„æµ‹ç¬¬10ä¸ªå°æ—¶çš„PM2.5å€¼, # è¿™æ ·è¿žç»­çš„ã€Œ10ä¸ªå°æ—¶ã€æ¯ä¸ªæœˆæœ‰471ä¸ª x.append([]) # for w in range(18): # éåŽ†18ä¸ªç‰¹å¾ for t in range(9): # éåŽ†å‰9ä¸ªå°æ—¶ x[471 * i + j].append(data[w][480 * i + j + t]) # å°†ç¬¬10ä¸ªå°æ—¶çš„å€¼ä½œä¸ºå®žé™…çš„PM2.5çš„å€¼ y.append(data[9][480 * i + j + 9])x = np.array(x)y = np.array(y)#åœ¨ç¬¬ä¸€åˆ—æ·»ä¸Šä¸€æ¡å…¨ä¸º1çš„åˆ—ä½œä¸ºbiasx = np.concatenate((np.ones((x.shape[0],1)),x),axis = 1) w = np.zeros(x.shape[1]) #weight è®­ç»ƒå®šä¹‰loss function ä½¿ç”¨ error square123456def lossFunction(target,weight,samples): M = target - np.dot(weight,samples.T) loss = 0 for m in M: loss += m**2 return loss Gradient Descent ä½¿ç”¨Adagra å¯¹learning rateè¿›è¡ŒæŽ§åˆ¶ 12345678910111213lr = 8 #learning rate è®¾ç½®pre_grad = np.ones(x.shape[1])# æ¯ä¸ªç‰¹å¾æœ‰ç‹¬ç«‹çš„learning ratefor r in range(10000): temp_loss = 0 for m in range(36): for s in range(156): L = np.dot(w,x[157 * m + s].T) - y[157 * m + s] grad = np.dot(x[157 * m + s].T,L)*(2) pre_grad += grad**2 ada = np.sqrt(pre_grad) w = w - lr * grad/ada temp_loss += abs(np.dot(w,x[157 * m + 156].T) - y[157 * m + 156]) print("%.2f" % (r * 100 / 10000),'% loss:',"%.4f" % (temp_loss / 36)) ä¿å­˜ weights1np.save('model.npy',w) æµ‹è¯• åŠ è½½æµ‹è¯•ç‰¹å¾æ•°æ®é›†(ç•¥) åŠ è½½label 123456789y = []rr = 0with open('ans.csv','r',encoding = 'big5') as ans: row = csv.reader(ans,delimiter = ',') for r in row: if rr != 0: y.append(float(r[1])) rr += 1y = np.array(y) åŠ è½½weights 1w = np.load('model.npy') æµ‹è¯• 123456789101112t = np.dot(x,w)L = t - yloss = []sum = 0for l in L: loss.append(abs(l)) sum += abs(l)print(sum / len(L))plt.plot(y,color = "red",label = 'target')plt.plot(t,color = "blue",label = 'hypothesis')plt.ylabel('pm 2.5')plt.show() ç»“æžœ PM2.5 è¯¯å·® 14.427å‚æ•°å¤ªå¤šè¿‡æ‹Ÿåˆæ²¡æœ‰è®­ç»ƒå¥½,å¡åœ¨äº†æŸä¸ªåœ°æ–¹äº†,trainingæ—¶å€™çš„lossä¹Ÿå¾ˆé«˜ å†ä¼˜åŒ– åªå–18ä¸ªç‰¹å¾ä¸­çš„NMHCã€NO2ã€O3ã€PM10ã€PM2.5 PM2.5 è¯¯å·® 8.987ä¸€ä¸ªä¸é”™çš„å¼€å¤´,ç»§ç»­ä¼˜åŒ– åªè€ƒè™‘PM 10å’ŒPM 2.5 PM2.5 è¯¯å·® 6.281 è‹¥åœ¨åˆ å‡ç‰¹å¾å‘¢?åªè€ƒè™‘ PM2.5 PM2.5 è¯¯å·® 5.406æˆ‘æœäº†,ä¹‹å‰åšçš„æ—¶å€™æ˜¯ä¼šunderfittingå¯¼è‡´è¯¯å·®åˆ°7.4çš„,è¿™å›žå€’å¥½æ›´åŠ ä½Žäº† ä½¿ç”¨DNN å¯¹PM2.5è¿›è¡Œé¢„æµ‹ ä½¿ç”¨çš„æ˜¯tensorflow + kerasé¢„å¤‡å·¥ä½œç•¥ Feature Scaling ç”¨äº†ä¸¤ä¸ªä¸åŒçš„Feature Scalingçš„æ–¹æ³•,ç»“æžœä¸Šçœ‹å·®åˆ«ä¸å¤§,Standardizationæ›´åŠ å¥½ä¸€ç‚¹ Standardization12345678910111213141516def standardization(dataMatrix): if dataMatrix.shape[0] == 0: return dataMatrix for i in range(dataMatrix.shape[1]): sum = 0 for _x in dataMatrix: sum += _x[i] mean = sum / dataMatrix.shape[0] SD = 0 for _x in dataMatrix: SD += (_x[i] - mean)**2 SD = np.sqrt(SD / dataMatrix.shape[0]) for _x in dataMatrix: _x[i] = (_x[i] - mean) / SD return dataMatrix Mean Normalization123456789101112131415161718def meanNormalization(dataMatrix): if dataMatrix.shape[0] == 0: return dataMatrix for i in range(dataMatrix.shape[1]): sum = 0 max = 0 min = 0 for data in dataMatrix: sum += data[i] if data[i] &gt; max: max = data[i] if data[i] &lt; min: min = data[i] mean = sum / dataMatrix.shape[0] if (max - min) != 0: for data in dataMatrix: data[i] = (data[i] - mean) / (max - min) return dataMatrix 1234trainX = standardization(trainX)testX = standardization(testX)# trainX = meanNormalization(trainX)# testX = meanNormalization(testX) è®­ç»ƒ ä½¿ç”¨outputä¸º8çš„ä¸¤å±‚layer,æ¿€æ´»å‡½æ•°æ˜¯ReLU(Sigmoidæ•ˆæžœæ›´åŠ å·®) å»ºç«‹æ¨¡åž‹12345678model = keras.Sequential([ keras.layers.Dense(8, activation=tf.nn.relu), keras.layers.Dense(8, activation=tf.nn.relu), keras.layers.Dense(1) ])model.compile(loss="mse", optimizer=tf.train.RMSPropOptimizer(0.001), metrics=['mae', 'mse']) è®­ç»ƒ è·‘100ä¸ªepochsåŸºæœ¬ä¸Šæ²¡ä»€ä¹ˆå˜åŒ–äº†1234567history = model.fit(trainX, trainY, batch_size = 64, epochs = 100, validation_split = 0.2, verbose=0, callbacks=[PrintDot()]) è®­ç»ƒæ—¶å€™çš„loss æµ‹è¯• lossæ˜¯7.47 å†ä¼˜åŒ–ä¸ä½¿ç”¨Feature Scaling è®­ç»ƒæ—¶å€™çš„loss æµ‹è¯•æ—¶lossæ˜¯5.170 å¢žåŠ layers å¢žåŠ å¤šä¸€å±‚layer è®­ç»ƒæ—¶loss æµ‹è¯•æ—¶loss 7.5 æ‰€ä»¥è¯´DNNæœ€ä¼˜èƒ½è¾¾åˆ°5.17, Linear Modelæœ€ä¼˜5.4ä¸é”™ðŸŽ‰ðŸŽ‰ðŸŽ‰ðŸŽ‰ðŸŽ‰ðŸŽ‰ RepoVIXNESS/machine-learning-course]]></content>
      <categories>
        <category>machine learning</category>
      </categories>
      <tags>
        <tag>machine learning</tag>
        <tag>linear regression</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Hello World]]></title>
    <url>%2F2018%2F12%2F23%2Fhello-world%2F</url>
    <content type="text"><![CDATA[Hello World! æˆ‘å›žæ¥å•¦~â•â•â•â•å¼€å§‹å†™ä½œ,æ›´æ–°blogå•¦ ðŸŽ‰ðŸŽ‰ðŸŽ‰ðŸŽ‰]]></content>
      <categories>
        <category>life</category>
      </categories>
      <tags>
        <tag>life</tag>
      </tags>
  </entry>
</search>
