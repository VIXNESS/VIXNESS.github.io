<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title><![CDATA[Effective Modern C++ 行为习惯列举]]></title>
    <url>%2F2019%2F05%2F07%2FEffective-Modern-C-%E8%A1%8C%E4%B8%BA%E4%B9%A0%E6%83%AF%E5%88%97%E4%B8%BE%2F</url>
    <content type="text"><![CDATA[Effective Modern C++ 行为习惯列举 面向已经掌握了C++11之前的同学, 本文只列举了基本的几条. ref: Effective Modern C++ 1.用auto 代替显示声明. ✖ 错误示范: 1typename std::iterator_traits&lt;It&gt;::value_type currValue = *b; ✔ 正确示范: 1auto currValue = *b; 2.使用nullptr 替代NULL 和0. ✖ 错误示范: 123if (result == 0)&#123; ...&#125; 123if (result == NULL)&#123; ...&#125; ✔ 正确示范: 123if (result == nullptr)&#123; ...&#125; 3.使用using 替代typedef. ✖ 错误示范: 1typedef std::unique_ptr&lt;std::unordered_map&lt;std::string, std::string&gt;&gt; UPtrMapSS; ✔ 正确示范: 1using UPtrMapSS = std::unique_ptr&lt;std::unordered_map&lt;std::string, std::string&gt;&gt;; 4.有范围的enums 替代无范围的enums. ✖ 错误示范: 12enum Color &#123; black, white, red &#125;;auto white = false; //white 已经被声明了, error! ✔ 正确示范: 1234enum class Color &#123; black, white, red &#125;;auto white = false; // 一切正常Color c = Color::white; //规范的声明方式auto c = Color::white; //规范的声明方式 5.禁用函数时, 用delete 替代private. ✖ 错误示范: 12345678class basic_ios : public ios_base &#123; public: ...private: basic_ios(const basic_ios&amp; ); basic_ios&amp; operator=(const basic_ios&amp;); ...&#125;; ✔ 正确示范: 123456class basic_ios : public ios_base &#123; public: basic_ios(const basic_ios&amp; ) = delete; basic_ios&amp; operator=(const basic_ios&amp;) = delete; ...&#125;; 6.使用override 关键字标注 override函数. 1234567891011121314class Base &#123; public: virtual void mf1() const; virtual void mf2(int x); virtual void mf3() &amp;; void mf4() const; &#125;;class Derived: public Base &#123; public: virtual void mf1() override; virtual void mf2(unsigned int x) override; virtual void mf3() &amp;&amp; override; virtual void mf4() const override;&#125;; 7.使用const_iterators 替代iterators. ✖ 错误示范: 1234std::vector&lt;int&gt; values;…auto it = std::find(values.begin(),values.end(), 1983); //使用begin()和end()values.insert(it, 1998); ✔ 正确示范: 1234std::vector&lt;int&gt; values; …auto it = std::find(values.cbegin(),values.cend(), 1983);//使用cbegin()和cend()values.insert(it, 1998); 8.如果函数不会抛出异常, 使用noexcept进行声明. ✖ C++98: 1int f(int x) throw(); ✔ C++11: 1int f(int x) noexcept; 9.使用智能指针 std::unique_ptr, std::shared_ptr, std::weak_ptr替代传统指针 (std::auto 淘汰了别用了). 10.能用constexpr就用constexpr. 11.让常成员函数线程安全: 使用std::mutex 或std::atomic 等. 12.善用右值[Rvalue], 语义转移[Move Semantics], 完美转发[Perfect Forwarding] 13.善用Lambda 表达式 14.善用并发编程API 15.容器中使用emplace_back(), 替代push_back()]]></content>
      <categories>
        <category>c++</category>
      </categories>
      <tags>
        <tag>c++</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[DNN 年收入预测]]></title>
    <url>%2F2018%2F12%2F24%2F%E5%B9%B4%E6%94%B6%E5%85%A5%E9%A2%84%E6%B5%8B%2F</url>
    <content type="text"><![CDATA[收入预测 通过 职位、婚姻情况、学历、家庭角色、种族、性别对其进行年薪的预测🥳🥳 Data SetTraining Data Testing Data 预备工作所需类库12345import tensorflow as tffrom tensorflow import kerasimport numpy as npimport sysimport csv 数据装载 数据构成: [feature 1, … feature n; &gt;= 50k or &lt; 50k] 😵😵😵123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177178179180181182def loadData(_x, _y, fileName): col_1 = [' State-gov', ' Self-emp-not-inc', ' Private', ' Federal-gov', ' Local-gov', ' ?', ' Self-emp-inc', ' Without-pay', ' Never-worked'] col_3 = [' Bachelors', ' HS-grad', ' 11th', ' Masters', ' 9th', ' Some-college', ' Assoc-acdm', ' Assoc-voc', ' 7th-8th', ' Doctorate', ' Prof-school', ' 5th-6th', ' 10th', ' 1st-4th', ' Preschool', ' 12th'] col_5 = [' Never-married', ' Married-civ-spouse', ' Divorced', ' Married-spouse-absent', ' Separated', ' Married-AF-spouse', ' Widowed'] col_6 = [' Adm-clerical', ' Exec-managerial', ' Handlers-cleaners', ' Prof-specialty', ' Other-service', ' Sales', ' Craft-repair', ' Transport-moving', ' Farming-fishing', ' Machine-op-inspct', ' Tech-support', ' ?', ' Protective-serv', ' Armed-Forces', ' Priv-house-serv'] col_7 = [' Not-in-family', ' Husband', ' Wife', ' Own-child', ' Unmarried', ' Other-relative'] col_8 = [' White', ' Black', ' Asian-Pac-Islander', ' Amer-Indian-Eskimo', ' Other'] col_9 = [' Male', ' Female'] col_13 = [' United-States', ' Cuba', ' Jamaica', ' India', ' ?', ' Mexico', ' South', ' Puerto-Rico', ' Honduras', ' England', ' Canada', ' Germany', ' Iran', ' Philippines', ' Italy', ' Poland', ' Columbia', ' Cambodia', ' Thailand', ' Ecuador', ' Laos', ' Taiwan', ' Haiti', ' Portugal', ' Dominican-Republic', ' El-Salvador', ' France', ' Guatemala', ' China', ' Japan', ' Yugoslavia', ' Peru', ' Outlying-US(Guam-USVI-etc)', ' Scotland', ' Trinadad&amp;Tobago', ' Greece', ' Nicaragua', ' Vietnam', ' Hong', ' Ireland', ' Hungary', ' Holand-Netherlands'] col_14 = [' &lt;=50K', ' &gt;50K', ' &lt;=50K.', ' &gt;50K.'] with open(fileName) as rawData: rows = csv.reader(rawData, delimiter = ",") for r in rows: if len(r) == 0: continue temp = [] for i in range(15): if i == 0: temp.append(float(r[i])) if i == 1: cnt = 0 for c in col_1: if c == r[i]: temp.append(float(cnt)) break cnt += 1 if i == 2: temp.append(float(r[i])) if i == 3: cnt = 0 for c in col_3: if c == r[i]: temp.append(float(cnt)) break cnt += 1 if i == 4: temp.append(float(r[i])) if i == 5: cnt = 0 for c in col_5: if c == r[i]: temp.append(float(cnt)) break cnt += 1 if i == 6: cnt = 0 for c in col_6: if c == r[i]: temp.append(float(cnt)) break cnt += 1 if i == 7: cnt = 0 for c in col_7: if c == r[i]: temp.append(float(cnt)) break cnt += 1 if i == 8: cnt = 0 for c in col_8: if c == r[i]: temp.append(float(cnt)) break cnt += 1 if i == 9: cnt = 0 for c in col_9: if c == r[i]: temp.append(float(cnt)) break cnt += 1 if i == 10 or i == 11 or i == 12: temp.append(float(r[i])) if i == 13: cnt = 0 for c in col_13: if c == r[i]: temp.append(float(cnt)) break cnt += 1 if i == 14: cnt = 0 for c in col_14: if c == r[i]: _y.append(float(cnt % 2)) break cnt += 1 _x.append(temp) 模型定义Feature Scaling 4种 Feature Scaling的方法 Standardization12345678910111213141516def standardization(dataMatrix): #🤩🤩🤩 if dataMatrix.shape[0] == 0: return dataMatrix for i in range(dataMatrix.shape[1]): sum = 0 for _x in dataMatrix: sum += _x[i] mean = sum / dataMatrix.shape[0] SD = 0 for _x in dataMatrix: SD += (_x[i] - mean)**2 SD = np.sqrt(SD / dataMatrix.shape[0]) for _x in dataMatrix: _x[i] = (_x[i] - mean) / SD return dataMatrix Mean Normalization123456789101112131415161718def meanNormalization(dataMatrix): if dataMatrix.shape[0] == 0: return dataMatrix for i in range(dataMatrix.shape[1]): sum = 0 max = 0 min = 0 for data in dataMatrix: sum += data[i] if data[i] &gt; max: max = data[i] if data[i] &lt; min: min = data[i] mean = sum / dataMatrix.shape[0] if (max - min) != 0: for data in dataMatrix: data[i] = (data[i] - mean) / (max - min) return dataMatrix Rescaling123456789101112131415def rescaling(dataMatrix): if dataMatrix.shape[0] == 0: return dataMatrix for i in range(dataMatrix.shape[1]): max = 0 min = 0 for data in dataMatrix: if data[i] &gt; max: max = data[i] if data[i] &lt; min: min = data[i] if max - min != 0: for data in dataMatrix: data[i] = (data[i] - min) / (max - min) return dataMatrix 使用以上某一种方法对数据进行Feature Scaling经过测试,Standardization的效果最好,其次是 Mean Normalization  12345678910111213141516trainX = []trainY = []testX = []testY = []loadData(trainX, trainY, 'train.csv')loadData(testX, testY, 'test.csv')trainX = np.array(trainX)trainY = np.array(trainY)testX = np.array(testX)testY = np.array(testY)trainX = standardization(trainX) #Test accuracy: 0.8504391622392502testX = standardization(testX)# trainX = meanNormalization(trainX) #Test accuracy: 0.8516675879786739# testX = meanNormalization(testX)# trainX = rescaling(trainX) #Test accuracy: 0.8477366255400303# testX = rescaling(testX) 训练设计模型 使用两层layers 激活函数 ReLU (就乱设计的) 😅Loss Function 用的是Cross Entropy, 因为使用了ReLU用Square Error会有许多的地方没有梯度,很尴尬🥵 12345678model = tf.keras.Sequential([ keras.layers.Dense(28, activation=tf.nn.relu), keras.layers.Dense(14, activation=tf.nn.relu), keras.layers.Dense(2,activation=tf.nn.softmax)])model.compile(optimizer=tf.train.AdamOptimizer(), loss='sparse_categorical_crossentropy', metrics=['accuracy']) 训练时的精准度: 测试精准度: 1216281/16281 [==============================] - 0s 25us/stepTest loss: 0.3315070253370084 Test accuracy: 0.8466310422863189 负优化取消Feature Scaling 很真实 🥵 1216281/16281 [==============================] - 0s 24us/stepTest loss: 12.310577790542826 Test accuracy: 0.23622627602315244 增加layers123456model = tf.keras.Sequential([ keras.layers.Dense(28, activation=tf.nn.relu), keras.layers.Dense(28, activation=tf.nn.relu), keras.layers.Dense(14, activation=tf.nn.relu), keras.layers.Dense(2,activation=tf.nn.softmax)]) 没用🤦‍ 🤦‍ 🤷‍ 🤷‍ 1216281/16281 [==============================] - 0s 26us/stepTest loss: 0.33134235454223626 Test accuracy: 0.8481665745274117 完结🎉🎉🎉 Repo&gt;50K?]]></content>
      <categories>
        <category>machine learning</category>
      </categories>
      <tags>
        <tag>machine learning</tag>
        <tag>deep learning</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[PM2.5 预测]]></title>
    <url>%2F2018%2F12%2F24%2FPM2-5%2F</url>
    <content type="text"><![CDATA[使用Linear Regression 对PM2.5进行预测数据集 training data testing data: samples testing data: label Training data 和 Public testing data 的组织形式: 一天由18行组成,一行为一个指标,一共由18个指标,从第4列开始记录每个指标一天内24小时的变化数值,每个月连续记录前20天作为training set,后10天作为testing set,一共记录了240个小时 日期 观测站 指标 0时 … 23时 day 1 xxx PM2.5 day 1 xxx PM10 day 1 xxx SO2 day 1 xxx … day 2 xxx PM2.5 day 2 xxx PM10 day 2 xxx SO2 day 2 xxx … 数据处理必要类库1234567import osimport tensorflow as tffrom tensorflow import kerasimport csvimport sysimport numpy as npimport matplotlib.pyplot as plt 载入数据12345678910111213141516data = []for i in range(18): data.append([]) # 初始化18列n_row = 0with open('train.csv','r',encoding = 'big5') as text: #csv编码是big5 row = csv.reader(text, delimiter = ",") for r in row: if n_row != 0: for i in range(3,27): if r[i] != "NR": #NR为未降雨,对其设置为降雨量 data[(n_row - 1) % 18].append(float(r[i])) else: #设施0降雨为一个接近0的小量, #若设为0,后续的梯度计算会有除0的风险 data[(n_row - 1) % 18].append(float(0.0001)) n_row += 1 重新组织数据 将之前的数据重新组织,对每个小时进行连续拼接 Features 0时 … 23时 0时(次日) … 23时 PM2.5 … .. PM 10 123456789101112131415161718x = [] # 样本矩阵y = [] # 实际的值for i in range(12): # 12个月 for j in range(471): # 每输入9个小时的数值,预测第10个小时的PM2.5值, # 这样连续的「10个小时」每个月有471个 x.append([]) # for w in range(18): # 遍历18个特征 for t in range(9): # 遍历前9个小时 x[471 * i + j].append(data[w][480 * i + j + t]) # 将第10个小时的值作为实际的PM2.5的值 y.append(data[9][480 * i + j + 9])x = np.array(x)y = np.array(y)#在第一列添上一条全为1的列作为biasx = np.concatenate((np.ones((x.shape[0],1)),x),axis = 1) w = np.zeros(x.shape[1]) #weight 训练定义loss function 使用 error square123456def lossFunction(target,weight,samples): M = target - np.dot(weight,samples.T) loss = 0 for m in M: loss += m**2 return loss Gradient Descent 使用Adagra 对learning rate进行控制 12345678910111213lr = 8 #learning rate 设置pre_grad = np.ones(x.shape[1])# 每个特征有独立的learning ratefor r in range(10000): temp_loss = 0 for m in range(36): for s in range(156): L = np.dot(w,x[157 * m + s].T) - y[157 * m + s] grad = np.dot(x[157 * m + s].T,L)*(2) pre_grad += grad**2 ada = np.sqrt(pre_grad) w = w - lr * grad/ada temp_loss += abs(np.dot(w,x[157 * m + 156].T) - y[157 * m + 156]) print("%.2f" % (r * 100 / 10000),'% loss:',"%.4f" % (temp_loss / 36)) 保存 weights1np.save('model.npy',w) 测试 加载测试特征数据集(略) 加载label 123456789y = []rr = 0with open('ans.csv','r',encoding = 'big5') as ans: row = csv.reader(ans,delimiter = ',') for r in row: if rr != 0: y.append(float(r[1])) rr += 1y = np.array(y) 加载weights 1w = np.load('model.npy') 测试 123456789101112t = np.dot(x,w)L = t - yloss = []sum = 0for l in L: loss.append(abs(l)) sum += abs(l)print(sum / len(L))plt.plot(y,color = "red",label = 'target')plt.plot(t,color = "blue",label = 'hypothesis')plt.ylabel('pm 2.5')plt.show() 结果 PM2.5 误差 14.427参数太多过拟合没有训练好,卡在了某个地方了,training时候的loss也很高 再优化 只取18个特征中的NMHC、NO2、O3、PM10、PM2.5 PM2.5 误差 8.987一个不错的开头,继续优化 只考虑PM 10和PM 2.5 PM2.5 误差 6.281 若在删减特征呢?只考虑 PM2.5 PM2.5 误差 5.406我服了,之前做的时候是会underfitting导致误差到7.4的,这回倒好更加低了 使用DNN 对PM2.5进行预测 使用的是tensorflow + keras预备工作略 Feature Scaling 用了两个不同的Feature Scaling的方法,结果上看差别不大,Standardization更加好一点 Standardization12345678910111213141516def standardization(dataMatrix): if dataMatrix.shape[0] == 0: return dataMatrix for i in range(dataMatrix.shape[1]): sum = 0 for _x in dataMatrix: sum += _x[i] mean = sum / dataMatrix.shape[0] SD = 0 for _x in dataMatrix: SD += (_x[i] - mean)**2 SD = np.sqrt(SD / dataMatrix.shape[0]) for _x in dataMatrix: _x[i] = (_x[i] - mean) / SD return dataMatrix Mean Normalization123456789101112131415161718def meanNormalization(dataMatrix): if dataMatrix.shape[0] == 0: return dataMatrix for i in range(dataMatrix.shape[1]): sum = 0 max = 0 min = 0 for data in dataMatrix: sum += data[i] if data[i] &gt; max: max = data[i] if data[i] &lt; min: min = data[i] mean = sum / dataMatrix.shape[0] if (max - min) != 0: for data in dataMatrix: data[i] = (data[i] - mean) / (max - min) return dataMatrix 1234trainX = standardization(trainX)testX = standardization(testX)# trainX = meanNormalization(trainX)# testX = meanNormalization(testX) 训练 使用output为8的两层layer,激活函数是ReLU(Sigmoid效果更加差) 建立模型12345678model = keras.Sequential([ keras.layers.Dense(8, activation=tf.nn.relu), keras.layers.Dense(8, activation=tf.nn.relu), keras.layers.Dense(1) ])model.compile(loss="mse", optimizer=tf.train.RMSPropOptimizer(0.001), metrics=['mae', 'mse']) 训练 跑100个epochs基本上没什么变化了1234567history = model.fit(trainX, trainY, batch_size = 64, epochs = 100, validation_split = 0.2, verbose=0, callbacks=[PrintDot()]) 训练时候的loss 测试 loss是7.47 再优化不使用Feature Scaling 训练时候的loss 测试时loss是5.170 增加layers 增加多一层layer 训练时loss 测试时loss 7.5 所以说DNN最优能达到5.17, Linear Model最优5.4不错🎉🎉🎉🎉🎉🎉 RepoVIXNESS/machine-learning-course]]></content>
      <categories>
        <category>machine learning</category>
      </categories>
      <tags>
        <tag>machine learning</tag>
        <tag>linear regression</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Hello World]]></title>
    <url>%2F2018%2F12%2F23%2Fhello-world%2F</url>
    <content type="text"><![CDATA[Hello World! 我回来啦~❕❕❕❕开始写作,更新blog啦 🎉🎉🎉🎉]]></content>
      <categories>
        <category>life</category>
      </categories>
      <tags>
        <tag>life</tag>
      </tags>
  </entry>
</search>
